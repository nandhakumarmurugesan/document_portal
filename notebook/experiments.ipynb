{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df86c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9b2ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2088e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd3eef45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80e37e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba4229c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to figure out what the capital of India is. Hmm, I'm not entirely sure, but I think it's a city that starts with an 'N'. Wait, is it New Delhi? That sounds familiar. I remember learning that India is a country in South Asia and that its capital is a well-known city. Let me think... I've heard of Mumbai and Bangalore, but those are more known for business and technology, right? So maybe the capital is different. \\n\\nI think New Delhi is the capital. I remember something about the government being there. But wait, isn't Delhi a state or a union territory? I think Delhi is a National Capital Territory, which includes New Delhi. So New Delhi is the capital city within that territory. That makes sense because sometimes countries have their capital as a specific city within a larger area. \\n\\nLet me double-check. I've heard of the Prime Minister of India, and I think they're based in New Delhi. Also, the Parliament House and other government buildings are there. So yeah, I'm pretty confident now that New Delhi is the capital. I don't think it's Mumbai because Mumbai is more of a financial hub, like how New York is in the US. \\n\\nWait, could it be another city? Maybe Kolkata or Chennai? No, those are major cities but not the capital. I'm pretty sure it's New Delhi. I remember seeing the India Gate and Red Fort in pictures, which are in New Delhi. Those are historical sites and important landmarks, so it makes sense that the capital would have such places. \\n\\nI guess sometimes people might confuse Mumbai as the capital because it's so prominent, but no, it's not. The capital is definitely New Delhi. I think that's right. I can't think of any other city in India that's known as the seat of the central government. So yeah, New Delhi must be the capital of India.\\n</think>\\n\\nThe capital of India is New Delhi. It is located within the National Capital Territory of Delhi and serves as the seat of the central government, housing key institutions such as the Parliament House and the official residence of the Prime Minister. New Delhi is distinguished from other major cities like Mumbai, which is a financial hub, and is known for landmarks like the India Gate and Red Fort. \\n\\n**Answer:** The capital of India is New Delhi.\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is the capital of India?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18db691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what is the capital of India?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dad7398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "583d6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "444287f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.042393896728754044,\n",
       " -0.03852992132306099,\n",
       " -0.04543685540556908,\n",
       " -0.010400405153632164,\n",
       " 0.057711824774742126,\n",
       " 0.018246879801154137,\n",
       " 0.010995607823133469,\n",
       " -0.03159891068935394,\n",
       " -0.0061469534412026405,\n",
       " 0.08103373646736145,\n",
       " -0.02868187427520752,\n",
       " 0.009125182405114174,\n",
       " -0.012654350139200687,\n",
       " 0.011821554973721504,\n",
       " -0.00645844079554081,\n",
       " 0.009969286620616913,\n",
       " 0.02616177871823311,\n",
       " 0.01606699638068676,\n",
       " 0.04204123839735985,\n",
       " -0.008058104664087296,\n",
       " 0.010650665499269962,\n",
       " 0.00743939820677042,\n",
       " -0.011419160291552544,\n",
       " 0.028364956378936768,\n",
       " 0.015575121156871319,\n",
       " -0.03722909465432167,\n",
       " -0.017734665423631668,\n",
       " -0.05043533071875572,\n",
       " -0.024109508842229843,\n",
       " 0.014887315221130848,\n",
       " -0.03904103487730026,\n",
       " 0.033777929842472076,\n",
       " -0.01970226690173149,\n",
       " 0.0062933615408837795,\n",
       " 0.026133719831705093,\n",
       " -0.06198522448539734,\n",
       " -0.020155781880021095,\n",
       " 0.023546874523162842,\n",
       " -0.011110058054327965,\n",
       " 0.05043229088187218,\n",
       " -0.012969013303518295,\n",
       " -0.021204402670264244,\n",
       " -0.08579286187887192,\n",
       " 0.014565164223313332,\n",
       " -0.014101285487413406,\n",
       " -0.00879749096930027,\n",
       " -0.050342947244644165,\n",
       " 0.02608552947640419,\n",
       " 0.009721603244543076,\n",
       " -0.014154705218970776,\n",
       " 0.006382461171597242,\n",
       " 0.0002023372653638944,\n",
       " 0.06301902234554291,\n",
       " -0.0021123175974935293,\n",
       " 0.003622582880780101,\n",
       " -0.047678034752607346,\n",
       " 0.0013860866893082857,\n",
       " -0.023954737931489944,\n",
       " 0.007704862859100103,\n",
       " 0.033016789704561234,\n",
       " 0.008591674268245697,\n",
       " -0.0045663765631616116,\n",
       " 0.015241480432450771,\n",
       " 0.05315668135881424,\n",
       " 0.008338128216564655,\n",
       " -0.04445282369852066,\n",
       " 0.004086456727236509,\n",
       " 0.012077506631612778,\n",
       " 0.07103536278009415,\n",
       " 0.019321678206324577,\n",
       " -0.010000644251704216,\n",
       " -0.04549195617437363,\n",
       " 0.064604252576828,\n",
       " -0.03905676677823067,\n",
       " 0.006187270861119032,\n",
       " -0.12195071578025818,\n",
       " -0.007144054863601923,\n",
       " 0.05300232395529747,\n",
       " 0.011550452560186386,\n",
       " -0.037100136280059814,\n",
       " 0.021557193249464035,\n",
       " -0.06348878890275955,\n",
       " -0.09656232595443726,\n",
       " -0.024084195494651794,\n",
       " -0.05000445619225502,\n",
       " 0.007725656498223543,\n",
       " -0.05747456103563309,\n",
       " 0.00039960569120012224,\n",
       " -0.015776455402374268,\n",
       " 0.042335063219070435,\n",
       " -0.016873061656951904,\n",
       " 0.02033706195652485,\n",
       " 0.02742110565304756,\n",
       " -0.04050980135798454,\n",
       " -0.013795779086649418,\n",
       " 0.10096028447151184,\n",
       " -0.017581745982170105,\n",
       " -0.02428724244236946,\n",
       " 0.04150695353746414,\n",
       " -0.005891601555049419,\n",
       " 0.03258221223950386,\n",
       " -0.03541871905326843,\n",
       " -0.07839832454919815,\n",
       " 0.03202085196971893,\n",
       " 0.04416721686720848,\n",
       " -0.0035023188684135675,\n",
       " 0.016266804188489914,\n",
       " 0.02542458102107048,\n",
       " -0.0002783752279356122,\n",
       " 0.05301372706890106,\n",
       " -0.05914405360817909,\n",
       " 0.002089941641315818,\n",
       " 0.03134347125887871,\n",
       " 0.031026691198349,\n",
       " 0.03490511327981949,\n",
       " -0.04629390686750412,\n",
       " -0.010582903400063515,\n",
       " -0.0007507766131311655,\n",
       " 0.02170315571129322,\n",
       " -0.007734865881502628,\n",
       " 0.05968312919139862,\n",
       " 0.00026084069395437837,\n",
       " 0.05282846465706825,\n",
       " -0.03189494460821152,\n",
       " 0.030886191874742508,\n",
       " -0.023902302607893944,\n",
       " -0.03378455340862274,\n",
       " 0.030488794669508934,\n",
       " -0.013045510277152061,\n",
       " 0.015316381119191647,\n",
       " -0.016252946108579636,\n",
       " -0.052330903708934784,\n",
       " -0.018754728138446808,\n",
       " -0.02488086000084877,\n",
       " 0.010920732282102108,\n",
       " 0.046060994267463684,\n",
       " 0.03152063116431236,\n",
       " -0.03436475619673729,\n",
       " 0.03276122733950615,\n",
       " -0.031087348237633705,\n",
       " 0.01589161716401577,\n",
       " 0.075983926653862,\n",
       " 0.0018717346247285604,\n",
       " 0.019573071971535683,\n",
       " 0.018951725214719772,\n",
       " 0.06750670075416565,\n",
       " -0.05127954110503197,\n",
       " -0.056834280490875244,\n",
       " 0.057058557868003845,\n",
       " -0.03339890390634537,\n",
       " -0.05409811809659004,\n",
       " -0.02534768171608448,\n",
       " -0.02056790515780449,\n",
       " -0.04030364751815796,\n",
       " 0.06207101047039032,\n",
       " -0.0038152728229761124,\n",
       " -0.03304009884595871,\n",
       " 0.029069306328892708,\n",
       " 0.003371715545654297,\n",
       " -0.001884712721221149,\n",
       " 0.0028990628197789192,\n",
       " 0.023274043574929237,\n",
       " 0.03377038612961769,\n",
       " 0.0008030089666135609,\n",
       " 0.009589338675141335,\n",
       " -0.042055536061525345,\n",
       " 0.031318292021751404,\n",
       " -0.008212017826735973,\n",
       " -0.008147460408508778,\n",
       " 0.035744186490774155,\n",
       " -0.02775993011891842,\n",
       " 0.005369660444557667,\n",
       " 0.0008108949405141175,\n",
       " -0.04535830393433571,\n",
       " 0.00859091803431511,\n",
       " -0.056127484887838364,\n",
       " 0.03323942795395851,\n",
       " -0.008228711783885956,\n",
       " -0.013998660258948803,\n",
       " -0.021932415664196014,\n",
       " 0.021296048536896706,\n",
       " -0.06976623833179474,\n",
       " 0.02289688028395176,\n",
       " 0.03842603415250778,\n",
       " -0.015894919633865356,\n",
       " -0.02342793345451355,\n",
       " 0.04018322750926018,\n",
       " -0.015009677037596703,\n",
       " -0.013783765956759453,\n",
       " 0.01229157391935587,\n",
       " -0.02250143140554428,\n",
       " -0.017225369811058044,\n",
       " -0.03742741048336029,\n",
       " -0.0419139564037323,\n",
       " -0.05130581557750702,\n",
       " -0.029545048251748085,\n",
       " 0.014767173677682877,\n",
       " -0.0006095857243053615,\n",
       " -0.0008035929640755057,\n",
       " -0.045472949743270874,\n",
       " -0.0333511121571064,\n",
       " 0.05269167944788933,\n",
       " 0.017855100333690643,\n",
       " -0.011428783647716045,\n",
       " 0.029565835371613503,\n",
       " -0.004175975918769836,\n",
       " 0.07525476068258286,\n",
       " -0.034971144050359726,\n",
       " -0.058018531650304794,\n",
       " 0.03230175003409386,\n",
       " -0.03581796959042549,\n",
       " 0.02037089131772518,\n",
       " -0.024321893230080605,\n",
       " 0.024437012150883675,\n",
       " 0.015305565670132637,\n",
       " -0.02822495810687542,\n",
       " 0.05800492689013481,\n",
       " 0.020505167543888092,\n",
       " 0.043050121515989304,\n",
       " -0.02258085459470749,\n",
       " -0.022962205111980438,\n",
       " 0.030825918540358543,\n",
       " -0.03697718307375908,\n",
       " -0.03479477018117905,\n",
       " 0.004797196015715599,\n",
       " 0.03912552446126938,\n",
       " -0.016966188326478004,\n",
       " -0.03907884657382965,\n",
       " 0.028220418840646744,\n",
       " -0.04276899993419647,\n",
       " -0.04960424825549126,\n",
       " 0.07319998741149902,\n",
       " 0.018495341762900352,\n",
       " -0.026795195415616035,\n",
       " 0.05907006934285164,\n",
       " 0.04113201051950455,\n",
       " 0.008972039446234703,\n",
       " 0.01827385276556015,\n",
       " 0.010972573421895504,\n",
       " 0.008454089984297752,\n",
       " -0.021943798288702965,\n",
       " -0.04740051180124283,\n",
       " 0.0610540471971035,\n",
       " 0.03670470044016838,\n",
       " -0.060285534709692,\n",
       " -0.0607653371989727,\n",
       " -0.03662046790122986,\n",
       " 0.03708244115114212,\n",
       " 0.03915243595838547,\n",
       " 0.05250796675682068,\n",
       " 0.013004479929804802,\n",
       " -0.016845686361193657,\n",
       " -0.003589941654354334,\n",
       " 0.050407469272613525,\n",
       " -0.0675496757030487,\n",
       " 0.0009554546559229493,\n",
       " -0.061209771782159805,\n",
       " 0.02198765240609646,\n",
       " -0.05026980862021446,\n",
       " 0.01793145202100277,\n",
       " 0.02375689707696438,\n",
       " 0.011922662146389484,\n",
       " 0.03239614889025688,\n",
       " -0.018776511773467064,\n",
       " 0.003632817417383194,\n",
       " -0.026942433789372444,\n",
       " -0.0027723798993974924,\n",
       " -0.06765995174646378,\n",
       " -0.0012161764316260815,\n",
       " 0.0018485912587493658,\n",
       " 0.024802470579743385,\n",
       " -0.07299104332923889,\n",
       " 0.04902004450559616,\n",
       " 0.021704066544771194,\n",
       " -0.005325726233422756,\n",
       " 0.021273836493492126,\n",
       " -0.03520205244421959,\n",
       " 0.06556987762451172,\n",
       " 0.04364878684282303,\n",
       " -0.07253330200910568,\n",
       " 0.037495601922273636,\n",
       " -0.0030056405812501907,\n",
       " -0.01926414482295513,\n",
       " -0.03419528529047966,\n",
       " -0.008285212330520153,\n",
       " 0.0056838723830878735,\n",
       " -0.06349841505289078,\n",
       " 0.006060535088181496,\n",
       " 0.02857683040201664,\n",
       " -0.02136852592229843,\n",
       " -0.030628520995378494,\n",
       " -0.09549514204263687,\n",
       " 0.014539756812155247,\n",
       " -0.03004443645477295,\n",
       " -0.054084550589323044,\n",
       " 0.016669578850269318,\n",
       " -0.040609657764434814,\n",
       " 0.05287715047597885,\n",
       " 0.03782205656170845,\n",
       " -0.019948290660977364,\n",
       " -0.028927050530910492,\n",
       " -0.0027579590678215027,\n",
       " 0.02030082978308201,\n",
       " -0.06465955078601837,\n",
       " 0.010793913155794144,\n",
       " 0.0049179932102561,\n",
       " -0.0416589118540287,\n",
       " -0.06782323122024536,\n",
       " 0.02106020599603653,\n",
       " 0.047703973948955536,\n",
       " 0.01341125089675188,\n",
       " -0.010421203449368477,\n",
       " -0.059272993355989456,\n",
       " 0.003882827702909708,\n",
       " -0.026917658746242523,\n",
       " 0.06070548668503761,\n",
       " -0.026613930240273476,\n",
       " -0.0015802354319021106,\n",
       " 0.009734547697007656,\n",
       " 0.04485827684402466,\n",
       " 0.00016400954336859286,\n",
       " 0.07268945872783661,\n",
       " 0.033177390694618225,\n",
       " 0.0020004797261208296,\n",
       " -0.0016924396622925997,\n",
       " 0.037829723209142685,\n",
       " 0.019927093759179115,\n",
       " 0.04088936373591423,\n",
       " 0.014363816939294338,\n",
       " -0.015340841375291348,\n",
       " -0.025766577571630478,\n",
       " 0.042909905314445496,\n",
       " -0.045303039252758026,\n",
       " -0.005072114523500204,\n",
       " 0.006580092012882233,\n",
       " 0.06089814752340317,\n",
       " -0.04668136313557625,\n",
       " 0.008919965475797653,\n",
       " -0.07343453913927078,\n",
       " 0.009393621236085892,\n",
       " 0.038678672164678574,\n",
       " 0.024896597489714622,\n",
       " -0.03097054548561573,\n",
       " -0.022735649719834328,\n",
       " -0.0050073121674358845,\n",
       " -0.009797527454793453,\n",
       " -0.03263060003519058,\n",
       " 0.02568226493895054,\n",
       " 0.09462129324674606,\n",
       " 0.008297266438603401,\n",
       " 0.016277551651000977,\n",
       " 0.0764932632446289,\n",
       " -0.003969029523432255,\n",
       " 0.03375404328107834,\n",
       " -0.012541763484477997,\n",
       " -0.048880137503147125,\n",
       " 0.05130503326654434,\n",
       " -0.0325680747628212,\n",
       " 0.009923392906785011,\n",
       " -0.06484595686197281,\n",
       " 0.01447070762515068,\n",
       " 0.02348277159035206,\n",
       " -0.0014077945379540324,\n",
       " -0.03523141145706177,\n",
       " -0.009411553852260113,\n",
       " -0.046120911836624146,\n",
       " -0.013744738884270191,\n",
       " 0.02220548875629902,\n",
       " 0.01975701004266739,\n",
       " 0.008533974178135395,\n",
       " 0.04364048317074776,\n",
       " 0.032175980508327484,\n",
       " 0.0527111291885376,\n",
       " -0.05161648616194725,\n",
       " 0.068728007376194,\n",
       " -0.00022764413733966649,\n",
       " -0.08461019396781921,\n",
       " -0.029739459976553917,\n",
       " 0.014554486609995365,\n",
       " 0.006212076637893915,\n",
       " -0.025818664580583572,\n",
       " -0.003423931309953332,\n",
       " 0.03697603940963745,\n",
       " 0.0059202248230576515,\n",
       " 0.011935221031308174,\n",
       " -0.018534965813159943,\n",
       " 0.023037100210785866,\n",
       " 0.03324737772345543,\n",
       " -0.033123746514320374,\n",
       " 0.04337380453944206,\n",
       " -0.033335600048303604,\n",
       " 0.03978458419442177,\n",
       " 0.07431113719940186,\n",
       " 0.007648700848221779,\n",
       " -0.004229549318552017,\n",
       " -0.024095505475997925,\n",
       " 0.012486797757446766,\n",
       " -0.016586149111390114,\n",
       " 0.026028834283351898,\n",
       " 0.026871656998991966,\n",
       " -0.005090073216706514,\n",
       " -0.08260282874107361,\n",
       " 0.004224834498018026,\n",
       " -0.018673282116651535,\n",
       " -0.051686886698007584,\n",
       " 0.008206386119127274,\n",
       " 0.006277198903262615,\n",
       " -0.02358207479119301,\n",
       " -0.043040741235017776,\n",
       " 0.019373543560504913,\n",
       " 0.021271441131830215,\n",
       " -0.012623623013496399,\n",
       " 0.03518449142575264,\n",
       " -0.0443604402244091,\n",
       " -0.04750466346740723,\n",
       " 0.002678659511730075,\n",
       " 0.03505973145365715,\n",
       " -0.025652019307017326,\n",
       " -0.0017427505226805806,\n",
       " 0.050865307450294495,\n",
       " -0.009904656559228897,\n",
       " -0.011627678759396076,\n",
       " -0.020031550899147987,\n",
       " 0.00920639093965292,\n",
       " -0.0951249971985817,\n",
       " -0.05739583447575569,\n",
       " -0.010872090235352516,\n",
       " 0.044828448444604874,\n",
       " 0.03399880975484848,\n",
       " -0.004297891166061163,\n",
       " 0.03485811501741409,\n",
       " -0.004009350202977657,\n",
       " -0.05530547350645065,\n",
       " -0.06363425403833389,\n",
       " -0.03387325257062912,\n",
       " -0.04600362852215767,\n",
       " 0.015215855091810226,\n",
       " 0.021994350478053093,\n",
       " -0.01168116182088852,\n",
       " -0.00950874574482441,\n",
       " -0.007582756690680981,\n",
       " -0.033247459679841995,\n",
       " 0.02487247996032238,\n",
       " 0.01915544830262661,\n",
       " -0.06046522408723831,\n",
       " 0.002938397927209735,\n",
       " -0.01242031529545784,\n",
       " 0.0073166824877262115,\n",
       " -0.003081101458519697,\n",
       " -0.04621310904622078,\n",
       " 0.05803337320685387,\n",
       " -0.021461715921759605,\n",
       " -0.031449511647224426,\n",
       " -0.04998842999339104,\n",
       " -0.083187535405159,\n",
       " -0.03153248503804207,\n",
       " -0.01671668142080307,\n",
       " 0.07943928241729736,\n",
       " -0.06163164973258972,\n",
       " 0.046574752777814865,\n",
       " 0.0032476403284817934,\n",
       " 0.004560008179396391,\n",
       " 0.013841218315064907,\n",
       " -0.11479426920413971,\n",
       " 0.05845499411225319,\n",
       " 0.02191365882754326,\n",
       " -0.0024613363202661276,\n",
       " 0.018318988382816315,\n",
       " 0.0442550852894783,\n",
       " 0.013447300530970097,\n",
       " 0.02815867029130459,\n",
       " -0.019328424707055092,\n",
       " 0.0026610600762069225,\n",
       " -0.020851818844676018,\n",
       " -0.04355654492974281,\n",
       " -0.02712933160364628,\n",
       " -0.07477099448442459,\n",
       " 0.04785497486591339,\n",
       " -0.04590659588575363,\n",
       " -0.03082977794110775,\n",
       " 0.0026208467315882444,\n",
       " 0.04513764753937721,\n",
       " 0.023975156247615814,\n",
       " 0.04769468680024147,\n",
       " -0.03427702188491821,\n",
       " 0.030009733512997627,\n",
       " -0.01626173034310341,\n",
       " -0.020259879529476166,\n",
       " -0.013564696535468102,\n",
       " 0.036940913647413254,\n",
       " 0.01752275601029396,\n",
       " -0.002804914955049753,\n",
       " -0.004522570874541998,\n",
       " -0.010161930695176125,\n",
       " -0.026053493842482567,\n",
       " -0.04130561649799347,\n",
       " -0.0013265340821817517,\n",
       " 0.04341888800263405,\n",
       " 0.03556984290480614,\n",
       " 0.030528316274285316,\n",
       " -0.023453054949641228,\n",
       " -0.03985891118645668,\n",
       " -0.02572220005095005,\n",
       " -0.054259415715932846,\n",
       " 0.06728440523147583,\n",
       " -0.07841306924819946,\n",
       " -0.010897665284574032,\n",
       " 0.0006755691720172763,\n",
       " -0.026583243161439896,\n",
       " -0.011927735060453415,\n",
       " 0.01573972962796688,\n",
       " 0.013276386074721813,\n",
       " 0.010528386570513248,\n",
       " 0.03406112268567085,\n",
       " 0.023060176521539688,\n",
       " 0.007111092563718557,\n",
       " 0.0017994132358580828,\n",
       " 0.03376660868525505,\n",
       " 0.04103926941752434,\n",
       " -0.007723273243755102,\n",
       " 0.001776443445123732,\n",
       " 0.010799936950206757,\n",
       " -0.08774437755346298,\n",
       " 0.010048077441751957,\n",
       " -0.01118231751024723,\n",
       " 0.000939657911658287,\n",
       " 0.0364019051194191,\n",
       " 0.03363511711359024,\n",
       " -0.05648062005639076,\n",
       " 0.0024894154630601406,\n",
       " -0.026053467765450478,\n",
       " 0.015261753462255001,\n",
       " -0.04244539886713028,\n",
       " -0.0332365408539772,\n",
       " 0.005620093550533056,\n",
       " -0.01808076538145542,\n",
       " -0.026355361565947533,\n",
       " 0.031117256730794907,\n",
       " 0.02621346153318882,\n",
       " -0.020653855055570602,\n",
       " 0.02023167908191681,\n",
       " -0.009263618849217892,\n",
       " 0.026737727224826813,\n",
       " 0.008530371822416782,\n",
       " -0.043916355818510056,\n",
       " 0.03557709604501724,\n",
       " -0.015541436150670052,\n",
       " -0.09057135134935379,\n",
       " -0.016809187829494476,\n",
       " 0.025231439620256424,\n",
       " -0.008261775597929955,\n",
       " 0.01770227774977684,\n",
       " 0.028760815039277077,\n",
       " -0.045603327453136444,\n",
       " -0.005348676815629005,\n",
       " -0.03678395599126816,\n",
       " -0.04223642498254776,\n",
       " 0.03868294879794121,\n",
       " 0.020002255216240883,\n",
       " -0.04938076063990593,\n",
       " -0.04411620646715164,\n",
       " 3.145761729683727e-05,\n",
       " -0.005994419567286968,\n",
       " 0.0402924083173275,\n",
       " 0.06815673410892487,\n",
       " 0.022671209648251534,\n",
       " -0.007263336330652237,\n",
       " -0.03555619344115257,\n",
       " 0.051335811614990234,\n",
       " -0.02135935239493847,\n",
       " -0.01850726269185543,\n",
       " 0.01392357051372528,\n",
       " 0.0506574846804142,\n",
       " 0.04372694715857506,\n",
       " 0.03176647052168846,\n",
       " -0.0322880744934082,\n",
       " -0.017514651641249657,\n",
       " -0.03012782894074917,\n",
       " -0.012317890301346779,\n",
       " -0.01723841205239296,\n",
       " 0.02085159346461296,\n",
       " -0.012889495119452477,\n",
       " 0.027647940441966057,\n",
       " 0.019335193559527397,\n",
       " -0.012942960485816002,\n",
       " 0.041779205203056335,\n",
       " 0.07753777503967285,\n",
       " 0.09049193561077118,\n",
       " 0.02246689423918724,\n",
       " 0.06370420008897781,\n",
       " -0.05868358165025711,\n",
       " 0.005889663007110357,\n",
       " -0.02630896493792534,\n",
       " 0.013812356628477573,\n",
       " 0.0354560911655426,\n",
       " 0.005868025589734316,\n",
       " -0.014697988517582417,\n",
       " -0.016701942309737206,\n",
       " -0.012896213680505753,\n",
       " -0.013487551361322403,\n",
       " 0.04726221784949303,\n",
       " -0.00849719624966383,\n",
       " -0.025994785130023956,\n",
       " -0.009326532483100891,\n",
       " 0.02458459883928299,\n",
       " 0.0727228969335556,\n",
       " -0.005700997076928616,\n",
       " 0.008864854462444782,\n",
       " -0.008309299126267433,\n",
       " 0.04937618598341942,\n",
       " 0.005409437697380781,\n",
       " -0.05397506803274155,\n",
       " 0.028545668348670006,\n",
       " -0.02686036191880703,\n",
       " -0.03630625456571579,\n",
       " -0.024997711181640625,\n",
       " 0.06975508481264114,\n",
       " -0.0238981693983078,\n",
       " 0.0059686144813895226,\n",
       " -0.017139598727226257,\n",
       " 0.004604377318173647,\n",
       " 0.004933697171509266,\n",
       " 0.014004628174006939,\n",
       " -0.02829611673951149,\n",
       " 0.022602640092372894,\n",
       " 0.03011454828083515,\n",
       " -0.01938645727932453,\n",
       " 0.005749857518821955,\n",
       " 0.0619509331882,\n",
       " 0.017579419538378716,\n",
       " 0.05011961981654167,\n",
       " 0.051960743963718414,\n",
       " -0.0073517258279025555,\n",
       " 0.01173106487840414,\n",
       " -0.0327427014708519,\n",
       " -0.028995754197239876,\n",
       " -0.04047673940658569,\n",
       " 0.005494239274412394,\n",
       " 0.009106206707656384,\n",
       " -0.012077554129064083,\n",
       " -0.04754888638854027,\n",
       " 0.006826556287705898,\n",
       " 0.0039288997650146484,\n",
       " -0.019900038838386536,\n",
       " 0.0023570084013044834,\n",
       " 0.10425996780395508,\n",
       " 0.03221553936600685,\n",
       " -0.07852212339639664,\n",
       " -0.05056200921535492,\n",
       " -0.024476943537592888,\n",
       " -0.01866176910698414,\n",
       " 0.0012895296094939113,\n",
       " 0.008390659466385841,\n",
       " -0.026248997077345848,\n",
       " 0.014815714210271835,\n",
       " -0.009648067876696587,\n",
       " -0.0485570952296257,\n",
       " -0.058417223393917084,\n",
       " 0.02722480706870556,\n",
       " 0.002418582560494542,\n",
       " -0.008080024272203445,\n",
       " -0.012796022929251194,\n",
       " -0.0009850113419815898,\n",
       " 0.0270937979221344,\n",
       " -0.029412446543574333,\n",
       " 0.0022397106513381004,\n",
       " -0.036433931440114975,\n",
       " -0.08336269110441208,\n",
       " -0.00929483026266098,\n",
       " 0.05002495273947716,\n",
       " -0.01158288586884737,\n",
       " 0.009410693310201168,\n",
       " 0.009788908995687962,\n",
       " -0.0014594100648537278,\n",
       " 0.057528458535671234,\n",
       " 0.028871377930045128,\n",
       " -0.012906895950436592,\n",
       " -0.010092528536915779,\n",
       " 0.04283803701400757,\n",
       " -0.005866400897502899,\n",
       " -0.0005346018588170409,\n",
       " 0.011116900481283665,\n",
       " -0.04617750272154808,\n",
       " 0.001101778238080442,\n",
       " 0.00528717739507556,\n",
       " 0.023569447919726372,\n",
       " 0.032811157405376434,\n",
       " -0.011693134903907776,\n",
       " -0.02778029255568981,\n",
       " -0.02846970409154892,\n",
       " 0.028870077803730965,\n",
       " -0.045624956488609314,\n",
       " -0.054070111364126205,\n",
       " 0.05405066907405853,\n",
       " 0.06547774374485016,\n",
       " 0.018491895869374275,\n",
       " 0.03393808379769325,\n",
       " -0.01006104052066803,\n",
       " -0.007463385351002216,\n",
       " 0.005935865920037031,\n",
       " -0.018001338467001915,\n",
       " -0.038622308522462845,\n",
       " -0.025915468111634254,\n",
       " 0.014714004471898079,\n",
       " 0.00010236084926873446,\n",
       " -0.013331781141459942,\n",
       " 0.026898376643657684,\n",
       " 0.044503044337034225,\n",
       " 0.048083167523145676,\n",
       " 0.059023842215538025,\n",
       " 0.041163370013237,\n",
       " -0.020552823320031166,\n",
       " -0.021909324452280998,\n",
       " 0.034874044358730316,\n",
       " 0.01379074715077877,\n",
       " -0.051375750452280045,\n",
       " 0.03110092133283615,\n",
       " 0.021003710106015205,\n",
       " -0.006296324543654919,\n",
       " 0.07419939339160919,\n",
       " 0.06852380931377411,\n",
       " 0.0528416745364666,\n",
       " 0.06444312632083893,\n",
       " 0.012551174499094486,\n",
       " -0.07186851650476456,\n",
       " 0.08519165217876434,\n",
       " -0.058429054915905,\n",
       " -0.019531918689608574,\n",
       " -0.007362003903836012,\n",
       " 0.0329291932284832,\n",
       " 0.0913722962141037,\n",
       " 0.023924581706523895,\n",
       " -0.01823551207780838,\n",
       " -0.003926971461623907,\n",
       " -0.006810910068452358,\n",
       " 0.09811940789222717,\n",
       " 0.02906220220029354,\n",
       " 0.019502494484186172,\n",
       " 0.012957768514752388,\n",
       " -0.08410349488258362,\n",
       " -0.023413144052028656,\n",
       " 0.008931766264140606,\n",
       " -0.012046456336975098,\n",
       " 0.03121325932443142,\n",
       " 0.024815568700432777,\n",
       " -0.026911534368991852,\n",
       " -0.008315605111420155,\n",
       " -0.018909631296992302,\n",
       " 0.01934615895152092,\n",
       " -0.10030505806207657,\n",
       " -0.03760595619678497,\n",
       " 0.004533902741968632,\n",
       " -0.03175932914018631,\n",
       " -0.014432201161980629,\n",
       " 0.03287350758910179,\n",
       " 0.023316893726587296,\n",
       " -0.01815115474164486,\n",
       " -0.0025647510774433613,\n",
       " 0.01124576199799776,\n",
       " -0.011457303538918495,\n",
       " -0.0045527685433626175,\n",
       " -0.03878457844257355,\n",
       " 0.04315076395869255,\n",
       " -0.018456850200891495,\n",
       " 0.018907826393842697,\n",
       " 0.035556819289922714,\n",
       " 0.02176062762737274,\n",
       " 0.02071385644376278]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query(\"what is the capital of India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e04413",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078c2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca369cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2709e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f49e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076babf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c71a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020e350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "587d3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed6dea28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8878ca",
   "metadata": {},
   "source": [
    "#Lets Chunk the data -- Not required for this small but experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22a25ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these values are based on test. No fixed/deterministic values\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=150, \n",
    "    length_function=len\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72e280ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b433f7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) #total number of chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5d8241a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87da4c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c8d7904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b05448e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "57475acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector= embedding_model.embed_documents(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7d99039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "795d5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c9f98586",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e468f3",
   "metadata": {},
   "source": [
    "#faiss is in memory vector store\n",
    "#faiss - on disk storage - persist over the disk, (chroma)\n",
    "#cloud storage  - cloud variant of faiss is not available [pinecone,weaviate,milvus,mongodbvectorsearch,astradb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "016059d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vectorstore.similarity_search(\"llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "580f7207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='11880948-9ac4-4cb5-988c-a9a869a6203b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='3b9cff8a-38cf-4868-8aa3-f28a68c032f3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='84a9b9e7-c500-49ef-9924-5e47846710b1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='04b1aa27-485b-411f-b3d2-885fa3d86a63', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "414e53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8b8ef51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "77c58ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "982ed3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2b1f0ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='11880948-9ac4-4cb5-988c-a9a869a6203b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='3b9cff8a-38cf-4868-8aa3-f28a68c032f3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='84a9b9e7-c500-49ef-9924-5e47846710b1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='04b1aa27-485b-411f-b3d2-885fa3d86a63', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d08c3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b3639d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='11880948-9ac4-4cb5-988c-a9a869a6203b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='3b9cff8a-38cf-4868-8aa3-f28a68c032f3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='84a9b9e7-c500-49ef-9924-5e47846710b1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='04b1aa27-485b-411f-b3d2-885fa3d86a63', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='88de0bb9-4187-472d-9dc4-12ce1f42c577', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='7f783753-8c78-4786-b849-078c1ec15584', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='8a271642-0964-46ec-9fcc-03f1f2c959ed', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 72, 'page_label': '73'}, page_content='Llama 2\\n7B 0.15 0.30 0.12 0.35 0.25 0.43 0.18 0.38 0.16 0.12 0.29 -0.1313B 0.14 0.35 0.23 0.29 0.23 0.57 0.20 0.52 0.22 0.12 0.29 -0.1734B 0.12 0.16 0.18 0.36 0.35 0.52 0.10 0.54 0.28 0.11 0.30 -0.1970B 0.16 0.21 0.17 0.35 0.30 0.60 0.18 0.67 0.26 0.12 0.30 -0.10\\nFine-tuned\\nChatGPT 0.15 0.22 0.05 0.24 0.31 0.35 0.09 0.42 0.19 0.09 0.23 0.06MPT-instruct 7B 0.13 0.29 0.12 0.34 0.35 0.53 0.28 0.56 0.27 0.02 0.32 -0.12Falcon-instruct 7B 0.11 0.21 0.21 0.28 0.34 0.23 0.31 0.45 0.23 0.22 0.29 -0.27'),\n",
       " Document(id='bd7e624d-d665-4d5b-b26c-90e197b54791', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(id='780453fc-99f7-414f-bf7a-2277f73de019', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'),\n",
       " Document(id='2dcbb9a9-01f4-4553-bcb3-9aa0c8d83387', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\91984\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61226bb1",
   "metadata": {},
   "source": [
    "#### Question: this is a user question\n",
    "#### Context: Based on the question retrieving the info from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4b020cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= \"\"\" \n",
    "    Answer the question based on the context provided below.\n",
    "    If the context does not sufficient information, respond with:\n",
    "    \"I do not have enough information about this.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e2f50230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c927fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9a3a649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=' \\n    Answer the question based on the context provided below.\\n    If the context does not sufficient information, respond with:\\n    \"I do not have enough information about this.\"\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "58cdba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4a526b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser() #convert output to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "11a2867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "81619621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5d6b5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e192f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m trying to figure out what the user is asking about. They mentioned \"llama2 finetuning benchmark experiments.\" From the context provided, I see that there are tables and some text discussing Llama 2 models, both in their base forms and fine-tuned versions.\\n\\nLooking at the context, I notice there\\'s a Table 3 that compares the overall performance of Llama 2 models with different sizes (like 7B, 13B, etc.) against other models. There are also some metrics listed, but I\\'m not exactly sure what each number represents. The text mentions that these models have undergone fine-tuning, specifically Supervised Fine-Tuning (SFT), which is detailed in section 3.1.\\n\\nI see that the results are grouped into categories, and each model\\'s performance is benchmarked across various tasks or datasets. The numbers in the tables could be scores or accuracy metrics across these benchmarks. There\\'s also mention of safety benchmarks in Section 4.1, which might be part of the fine-tuning experiments.\\n\\nThe user wants to know about the experiments conducted for fine-tuning Llama 2. I should explain what fine-tuning involves, perhaps mention the different model sizes, and highlight the performance improvements or key findings from the benchmarks. I should also refer to the specific tables and sections in the context that discuss these experiments.\\n\\nI need to make sure I don\\'t provide incorrect information, so I\\'ll stick to what\\'s in the context. I\\'ll structure the answer by first introducing what fine-tuning is, then describe the experiments as per the context, and mention the performance results from the tables.\\n</think>\\n\\nThe fine-tuning benchmark experiments for Llama 2, as detailed in the provided context, involve evaluating the model\\'s performance after undergoing Supervised Fine-Tuning (SFT). These experiments are discussed in section 3.1 of the document. \\n\\nThe experiments compare the performance of Llama 2 models of various sizes (7B, 13B, 34B, 70B) against other models, including fine-tuned versions like ChatGPT. The results are presented in tables that group benchmarks into different categories, such as academic and safety benchmarks. \\n\\nKey findings include:\\n1. **Performance Improvements**: Fine-tuning significantly enhances the model\\'s performance across various tasks. For instance, the 70B model shows notable improvements in benchmarks, achieving higher scores compared to its base counterparts.\\n2. **Benchmark Comparisons**: The models are evaluated on multiple benchmarks, with results indicating competitive performance. The context highlights that Llama 2 models often outperform or match other state-of-the-art models in these tasks.\\n3. **Safety Benchmarks**: Safety is a critical aspect, with specific benchmarks discussed in Section 4.1, showing how fine-tuning impacts the model\\'s safety and ethical behavior.\\n\\nThese experiments demonstrate the effectiveness of fine-tuning in enhancing Llama 2\\'s capabilities across diverse tasks, making it a robust model for various applications.'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell me about the llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f73d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
